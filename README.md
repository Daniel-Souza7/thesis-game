# Optimal Stopping Game

An interactive web application where players compete against a machine learning algorithm on optimal stopping problems for American-style exotic options. This project accompanies a Master's thesis on pricing financial derivatives with early exercise features using randomized neural networks and Monte Carlo methods.

**Live Demo**: [Play the Game](https://linktr.ee/salvadorsouza)

## Table of Contents

- [Introduction](#introduction)
- [How to Play](#how-to-play)
- [Game Products](#game-products)
- [Theoretical Background](#theoretical-background)
  - [Optimal Stopping Problem](#optimal-stopping-problem)
  - [The RT Algorithm](#the-rt-algorithm)
  - [The Rough Heston Model](#the-rough-heston-model)
- [Repository Structure](#repository-structure)
- [Installation](#installation)
- [Deployment](#deployment)
- [Author](#author)

---

## Introduction

The valuation of American-style options presents one of the most challenging problems in quantitative finance. Unlike European options that can only be exercised at maturity, American options grant the holder the right to exercise at any time up to expiration. This early exercise feature transforms the pricing problem into an **optimal stopping problem**—determining the strategy that maximizes the expected discounted payoff.

This web application demonstrates the practical application of machine learning algorithms to solve high-dimensional optimal stopping problems. Players challenge a trained neural network algorithm across nine different exotic option products, ranging from simple barrier calls to complex path-dependent derivatives with moving barriers.

The machine opponent uses the **RT algorithm** (Randomized Neural Networks for Optimal Stopping), trained on millions of simulated stock paths generated by the **Rough Heston model**—a stochastic volatility model calibrated to real S&P 500 options data.

---

## How to Play

### Objective

Decide when to exercise your option to maximize your payoff. You compete against a pre-trained algorithm. Can you beat the machine?

### Game Flow

1. **Observe**: Watch the stock price(s) evolve step by step over 12 time periods (representing monthly observations over one year)
2. **Decide**: At each time step, choose one of two actions:
   - **HOLD**: Continue to the next time step, hoping for a more favorable price movement
   - **EXERCISE**: Lock in your current payoff and end the game
3. **Compare**: After you make your decision, the machine's decision for that step is revealed
4. **Win**: Achieve a higher payoff than the algorithm

### Important Rules

- **Barrier Events**: If any stock price breaches a barrier level, the option is knocked out and your payoff becomes $0
- **Machine Decisions**: The AI's decision is only revealed after you make yours—no peeking!
- **Maturity**: If you hold until the final step (t=12), the option automatically exercises at the terminal payoff
- **Same Path**: Both you and the machine see the exact same price path, but the machine applies a pre-learned optimal strategy

### Difficulty Levels

| Level | Description |
|-------|-------------|
| **Medium** | Simple barrier options with straightforward payoffs |
| **Hard** | Moving barriers or more complex multi-asset payoffs |
| **Impossible** | Exotic path-dependent options with multiple features |

---

## Game Products

### Medium Difficulty

| Game | Stocks | Payoff | Barrier |
|------|--------|--------|---------|
| **Up-and-Out Call** | 1 | max(S - K, 0) | Upper at 120 |
| **Down-and-Out Min Put** | 3 | max(K - min(S₁,S₂,S₃), 0) | Lower at 85 |
| **Double Barrier Max Call** | 7 | max(max(S₁,...,S₇) - K, 0) | Upper at 130, Lower at 85 |

### Hard Difficulty

| Game | Stocks | Payoff | Barrier |
|------|--------|--------|---------|
| **Randomly Moving Barrier Call** | 1 | max(S - K, 0) | Upper moves randomly each step |
| **Up-and-Out Min Put** | 3 | max(K - min(S₁,S₂,S₃), 0) | Upper at 120 |
| **Down-and-Out Best-of-2 Call** | 7 | max(avg(top 2 stocks) - K, 0) | Lower at 85 |

### Impossible Difficulty

| Game | Stocks | Payoff | Barrier |
|------|--------|--------|---------|
| **Double Barrier Lookback Put** | 1 | max(max₀≤τ≤t S(τ) - S(t), 0) | Upper at 115, Lower at 85 |
| **Double Barrier Rank-Weighted Call** | 3 | max(0.15·S₍₁₎ + 0.50·S₍₂₎ + 0.35·S₍₃₎ - K, 0) | Upper at 125, Lower at 80 |
| **Double Moving Barrier Dispersion** | 7 | max(σ(S₁,...,S₇) - 1, 0) | Both barriers move randomly |

Where K = 100 (strike price), S₍ᵢ₎ denotes the i-th ranked stock, and σ denotes the standard deviation across stocks.

---

## Theoretical Background

### Optimal Stopping Problem

Consider a financial derivative with payoff function g(t, Xₜ) that can be exercised at any time t ∈ {0, 1, ..., T}. The optimal stopping problem seeks to find:

```
V₀ = sup{τ} E[e^(-rτ) g(τ, Xτ)]
```

where τ is a stopping time adapted to the filtration generated by the stock price process X, and r is the risk-free rate.

The solution involves **backward induction**: at each time step t, the holder compares the immediate exercise value g(t, Xₜ) against the continuation value C(t, Xₜ)—the expected discounted value of holding the option. The optimal strategy exercises when:

```
g(t, Xₜ) ≥ C(t, Xₜ)
```

### The RT Algorithm

The RT algorithm (Randomized Neural Networks for Optimal Stopping) extends the classical Longstaff-Schwartz method by using randomized neural networks to approximate continuation values.

#### Key Features

1. **Randomized Hidden Layer**: The neural network has a single hidden layer with randomly initialized weights that remain frozen during training. Only the output layer weights are learned via least-squares regression.

2. **Reservoir Computing Architecture**:
   ```
   φ(x) = σ(Wx + b)
   ```
   where W and b are random matrices, and σ is the GELU activation function.

3. **Backward Induction**: Starting from maturity T-1 and working backward:
   - Compute immediate exercise values: g(t, Xₜ)
   - Approximate continuation values using basis function expansion
   - Learn regression coefficients via least squares on in-the-money paths

4. **Payoff-Augmented Input**: The current payoff value is included as an input feature, providing the network with information about moneyness.

#### Training Configuration

| Parameter | Value |
|-----------|-------|
| Training paths | 15,000,000 (1-stock), 5,000,000 (3-stock), 2,000,000 (7-stock) |
| Hidden neurons | 40 |
| Activation | GELU |
| Time steps | 12 |
| Train ITM only | Yes |
| Use payoff as input | Yes |

### The Rough Heston Model

Stock paths are generated using the Rough Heston model, which captures the empirically observed rough behavior of volatility (H ≈ 0.1 rather than H = 0.5 for standard Brownian motion).

#### Model Dynamics

The stock price S and variance V follow:

```
dSₜ = μSₜdt + √Vₜ Sₜ dWₜ

Vₜ = V₀ + (1/Γ(H+½)) ∫₀ᵗ (t-s)^(H-½) [λ(θ - Vₛ)ds + ν√Vₛ dZₛ]
```

where dWₜ and dZₜ are correlated Brownian motions with correlation ρ.

#### Calibrated Parameters

Based on "SPX Calibration of Option Approximations under Rough Heston Model" (Jeng & Kilicman, 2021):

| Parameter | Symbol | Value | Description |
|-----------|--------|-------|-------------|
| Hurst exponent | H | 0.03 | Controls roughness (< 0.5 = rough) |
| Initial variance | V₀ | 0.026 | Starting volatility level |
| Long-term variance | θ | 0.07 | Mean reversion target |
| Mean reversion speed | λ | 0.5 | Rate of variance mean reversion |
| Vol-of-vol | ν | 0.29 | Volatility of variance |
| Correlation | ρ | -0.75 | Leverage effect (negative) |
| Drift | μ | 0.02 | Expected return |
| Initial spot | S₀ | 100 | Starting stock price |

The low Hurst parameter (H = 0.03 << 0.5) produces the jagged, non-smooth volatility paths observed in real markets, making optimal stopping decisions more challenging.

---

## Repository Structure

```
thesis-game/
├── api/
│   └── index.py                    # Vercel serverless function entry point
├── backend/
│   ├── algorithms/
│   │   ├── rt.py                   # RT algorithm (PyTorch, training)
│   │   ├── rt_numpy.py             # RT inference (NumPy only, deployment)
│   │   └── utils/
│   │       ├── randomized_neural_networks.py  # PyTorch reservoir
│   │       └── reservoir_numpy.py             # NumPy reservoir
│   ├── models/
│   │   ├── rough_heston.py         # Rough Heston path generator
│   │   └── black_scholes.py        # GBM path generator (alternative)
│   ├── payoffs/
│   │   └── game_payoffs.py         # All 9 game payoff functions
│   ├── data/
│   │   ├── trained_models/         # Pre-trained .pkl files (~90 KB)
│   │   └── paths/                  # Test paths .npz files (~230 KB)
│   ├── train_models.py             # Full training script
│   ├── retrain_models.py           # Retrain with existing paths
│   ├── generate_test_paths.py      # Generate test paths
│   ├── verify_models.py            # Verify model predictions
│   └── api.py                      # Flask API server
├── frontend/
│   ├── src/
│   │   ├── components/
│   │   │   ├── GameBoard.jsx       # Main game logic
│   │   │   ├── StockChart.jsx      # Price chart visualization
│   │   │   ├── InfoPanel.jsx       # Game information display
│   │   │   ├── ControlPanel.jsx    # Hold/Exercise buttons
│   │   │   ├── ResultsDisplay.jsx  # End game results
│   │   │   └── GameSelectionModal.jsx
│   │   ├── styles/
│   │   │   └── index.css           # Retro arcade styling
│   │   ├── App.jsx                 # Main React component
│   │   └── main.jsx                # React entry point
│   ├── public/
│   │   └── info.html               # Game information page
│   ├── package.json
│   └── vite.config.js
├── vercel.json                     # Vercel deployment configuration
├── requirements.txt                # Python dependencies
└── README.md
```

---

## Installation

### Prerequisites

- Python 3.9+
- Node.js 18+
- PyTorch (for training only; not required for deployment)

### Local Development

1. **Clone the repository**:
   ```bash
   git clone https://github.com/yourusername/thesis-game.git
   cd thesis-game
   ```

2. **Install Python dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Install frontend dependencies**:
   ```bash
   cd frontend
   npm install
   cd ..
   ```

4. **Train models** (optional—pre-trained models are included):
   ```bash
   python backend/train_models.py
   ```

5. **Start the backend server**:
   ```bash
   python backend/api.py
   ```
   Server runs on `http://localhost:5000`

6. **Start the frontend** (in a new terminal):
   ```bash
   cd frontend
   npm run dev
   ```
   App runs on `http://localhost:3000`

### Verify Installation

```bash
# Test backend health
curl http://localhost:5000/api/health

# Test game start
curl "http://localhost:5000/api/game/start?product=upandoutcall"
```

---

## Deployment

The application is deployed on **Vercel** as a unified stack:

- **Frontend**: React/Vite static build served via CDN
- **Backend**: Flask API running as Python serverless functions
- **Data**: Pre-trained models and test paths bundled in deployment (~320 KB total)

### Deploy to Vercel

```bash
# Install Vercel CLI
npm install -g vercel

# Deploy
vercel --prod
```

### Performance

| Metric | Value |
|--------|-------|
| Cold start | 2-4 seconds |
| Warm request | 200-500 ms |
| Model size | ~90 KB (all 9 games) |
| Test paths | ~230 KB |

---

## Author

**Daniel Souza**
Master's Student in Quantitative Finance

This project was developed as part of a Master's thesis on applying machine learning techniques to solve optimal stopping problems in quantitative finance. The research focuses on pricing American-style exotic options under realistic market conditions using randomized neural networks and Monte Carlo simulation.

For more information about the thesis and related documents, visit: [linktr.ee/salvadorsouza](https://linktr.ee/salvadorsouza)

---

## License

This project is developed for academic purposes as part of Master's thesis research.
